{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54482cf5-7946-4604-8b8d-32e33ab172ae",
   "metadata": {},
   "source": [
    "# Intelligent Architectures (5LIL0) Assignment 2 (version 0.2)\n",
    "\n",
    "#### Author: Alexios Balatsoukas-Stimming, License: [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
    "\n",
    "## Part A: Optimization of convolutional neural network using quantization and pruning\n",
    "\n",
    "In this notebook, you will first implement and train a convolutional neural network (CNN) for the MNIST dataset using PyTorch. Then, you will learn how to optimize this neural network using quantization and pruning.\n",
    "\n",
    "Let us first import the required packages and the dataset, as in the previous assignment. Note that we use the ``transforms`` class from the ``torchvision`` package to normalize the dataset, instead of doing it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dfdb36-5bae-4cce-a9f6-bd29e1cf6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_CBWR\"] = \"COMPATIBLE\"\n",
    "\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quantize\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision import datasets, transforms \n",
    "import timeit\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load and normalize the dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0, 1)\n",
    "    ])\n",
    "mnist_train_set = datasets.MNIST('data', train=True, download = True, transform=transform)\n",
    "mnist_test_set = datasets.MNIST('data', train=False, download = True, transform=transform)\n",
    "mnist_train_set = list(mnist_train_set)\n",
    "mnist_test_set = list(mnist_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b6f1d-e15a-43c0-a031-d4277a4c1f7f",
   "metadata": {},
   "source": [
    "## 1. A simple CNN for the MNIST dataset\n",
    "\n",
    "### 1.1 Define the CNN architecture\n",
    "\n",
    "To define a CNN, we use [`nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) that implements the typical convolutional layer we have discussed in the lectures. Here, we use a simple CNN with one convolutional layer with a single input channel of size ``28``×``28`` and with ``out_chans`` output channels (whose size depends on the filter size, the stride, and the padding). The convolutional layer is followed by a maxpooling layer with filter size ``maxpool_size`` and a fully connected layer with ``10`` outputs. The convolutional and fully connected layers are initialized with uniformly distributed random numbers by default.\n",
    "\n",
    "First, we define a helper function to calculate the output size of a convolutional layer with a given input size, filter size, stride, and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a974743c-bcd0-4d23-b540-039738035adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_size(input_size,filter_size,stride,padding):\n",
    "    output_size = ((input_size + 2 * padding - filter_size) / stride) + 1\n",
    "    return int(output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fa021-59b6-4c6c-9e2e-ef61d8e4c0d9",
   "metadata": {},
   "source": [
    "The max pooling layer output size ``max_pool_size`` can be calculated in the same way as for a convolutional layer by setting the input size equal to the output size of the previous convolutional layer (i.e., ``conv_out_size``, which also needs to be calculated), the stride equal to the filter size, and the padding to zero. The total number of features after max pooling is``out_chans``×``maxpool_out_size``×``maxpool_out_size``, so the fully connected layer that follows it needs to have that number of input neurons. Moreover, the tensor that is output by the convolutional layer has dimensions ``out_chans``×``out_size``×``out_size`` (per batch), but the fully connected layer expects a tensor if dimensions ``1``×``out_chans``×``out_size``×``out_size`` so we need to use the ``reshape`` method to reshape the tensor before feeding it into the fully connected layer.\n",
    "\n",
    "**Note:** you can ignore all commands containing the \"quant\" keyword, these will be used later for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d025d-a27d-4c84-abcc-e98e0201115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your convolutional layer output size is 14\n",
      "Correct convolutional layer output size is 14\n",
      "Your maxpool layer output size is 7\n",
      "Correct maxpool layer output size is 7\n"
     ]
    }
   ],
   "source": [
    "class MNISTCNN(nn.Module):\n",
    "    def __init__(self,out_chans,filter_size,stride,padding,maxpool_size,quant_flag=False):\n",
    "        super(MNISTCNN, self).__init__()\n",
    "        # Define sizes\n",
    "        in_size = 28\n",
    "        self.out_chans = out_chans\n",
    "        self.quant_flag = quant_flag\n",
    "        # Calculate the size of the convolutional and maxpool layers (use int() to cast to an integer)\n",
    "        self.conv_out_size = calc_output_size(in_size,filter_size,stride,padding)\n",
    "        self.maxpool_out_size = calc_output_size(self.conv_out_size,maxpool_size,stride,0)\n",
    "        # Define the layers\n",
    "        self.conv = nn.Conv2d(1, out_chans, filter_size, stride, padding)\n",
    "        self.maxpool = nn.MaxPool2d(maxpool_size)\n",
    "        self.fc = nn.Linear(int(out_chans*self.maxpool_out_size*self.maxpool_out_size), 10)\n",
    "        if self.quant_flag:\n",
    "            self.quant = quantize.QuantStub()\n",
    "            self.dequant = quantize.DeQuantStub()\n",
    "    def forward(self, x):\n",
    "        if self.quant_flag:\n",
    "            x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(-1, int(self.out_chans*self.maxpool_out_size*self.maxpool_out_size))\n",
    "        x = self.fc(x)\n",
    "        if self.quant_flag:\n",
    "            x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate CNN\n",
    "out_chans = 16\n",
    "filter_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "maxpool_size = 2\n",
    "model = MNISTCNN(out_chans,filter_size,stride,padding,maxpool_size,quant_flag=False)\n",
    "print(f\"Your convolutional layer output size is {model.conv_out_size}\")\n",
    "print(f\"Correct convolutional layer output size is 14\")\n",
    "print(f\"Your maxpool layer output size is {model.maxpool_out_size}\")\n",
    "print(f\"Correct maxpool layer output size is 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df948af2-0359-4db3-9b09-803e56f7bc10",
   "metadata": {},
   "source": [
    "### 1.2 Train the CNN\n",
    "\n",
    "As in the previous assignment, we first define the training function and an accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f377f-72a4-446e-aff3-0318986d8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_loader,criterion,optimizer,batch_size,num_epochs,log):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clean-up step for each epoch\n",
    "        accuracy_train = 0\n",
    "        accuracy_test  = 0\n",
    "\n",
    "        # Forward pass, loss, backward pass, and gradient descent step\n",
    "        model.train()\n",
    "        for _, (imgs, labels) in enumerate(train_loader):\n",
    "            out = model(imgs)             \n",
    "            loss = criterion(out, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()   \n",
    "            _, preds = torch.max(out.data, 1) # get the index of the max log-probability \n",
    "            accuracy_train += (preds== labels).sum().item()/labels.shape[0]/len(train_loader)\n",
    "\n",
    "        # Calculate test set accuracy\n",
    "        model.eval()\n",
    "        for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "            out_test = model(imgs_test)\n",
    "            _, preds = torch.max(out_test.data, 1) # get the index of the max log-probability \n",
    "            accuracy_test += (preds== labels_test).sum().item()/labels_test.shape[0]/len(test_loader)\n",
    "\n",
    "        if(log):\n",
    "            print(f\"Epoch {epoch+1}: train_acc = {accuracy_train:.3f} | test_acc = {accuracy_test:.3f}\" )\n",
    "\n",
    "def accuracy(model,loader):\n",
    "    accuracy = 0\n",
    "    for _, (imgs, labels) in enumerate(loader):    \n",
    "        out = model(imgs)\n",
    "        _, preds = torch.max(out.data, 1) # get the index of the max log-probability \n",
    "        accuracy += (preds== labels).sum().item()/labels.shape[0]/len(loader)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef8a00-e015-4703-90ea-ab7ae9fc4068",
   "metadata": {},
   "source": [
    "Then, we instantiate and train the CNN using the Adam optimizer with ``lr=0.002`` and a cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9385e7d-7a2a-4197-8c61-738e4b5e2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Instantiate CNN\n",
    "out_chans = 16\n",
    "filter_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "maxpool_size = 2\n",
    "model = MNISTCNN(out_chans,filter_size,stride,padding,maxpool_size,quant_flag=False)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "torch.manual_seed(0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "log=True\n",
    "\n",
    "# Dataset loaders\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train_set, batch_size=batch_size,shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(mnist_test_set, batch_size=batch_size)\n",
    "\n",
    "# Run training\n",
    "train(model,train_loader,test_loader,criterion,optimizer,batch_size,num_epochs,log)\n",
    "\n",
    "print(f\"Indicative test set accuracy after 10 epochs: 0.975\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa7155-50c4-428f-adc0-c9f532278d8c",
   "metadata": {},
   "source": [
    "## 2. Quantization\n",
    "\n",
    "### 2.1 Post-training quantization (PTQ)\n",
    "\n",
    "We will now apply post-training quantization to the model trained in the previous section. First, we need to re-instantiate our model to explicitly state where quantization and de-quantization will take place using ``QuantStub()`` and ``DeQuantStub()`` in the original definition of the ``MNISTCNN`` class, even though no quantization takes place yet. In this example, we want the input image to be quantized, all operations to be carried out with quantized values, and then the output can be dequantized. Then, we copy the previously trained weights to this new model to avoid re-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c03ea-3d4b-4ac4-a984-6bedc03a6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-instantiate CNN\n",
    "out_chans = 16\n",
    "filter_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "maxpool_size = 2        \n",
    "model_fp = MNISTCNN(out_chans,filter_size,stride,padding,maxpool_size,quant_flag=True)\n",
    "\n",
    "# Copy weights from previously trained model\n",
    "model_fp.load_state_dict(model.state_dict())\n",
    "\n",
    "# Calculate accuracy of floating-point model on test set\n",
    "accuracy_fp = accuracy(model_fp,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b06789-97c8-48cb-a0db-0383b6d2ec44",
   "metadata": {},
   "source": [
    "The next step is to define the type of quantization we want using a ``QConfig`` object. The quantization can be configured separately for activations and for weights, which is useful when using ReLU-type activations since they are aways non-negative while weights can generally take on any real value. We will use a ``MinMaxObserver`` that collects statistics for the minimum and maximum values and the default per-tensor asymmetric quantization (i.e., ``qscheme=per_tensor_affine``), but per-channel and symmetric options are also available. Activations will be stored in 8-bit unsigned integer variables and weights will be stored in 8-bit signed integer variables. We can also simulate smaller bit-widths by setting ``quant_min`` and ``quant_max`` appropriately as a function of the bit-width variable ``bits`` (**Careful:** unsigned and signed integers have different ranges for the same bit-width!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e7daf-e05c-4838-9ce2-c8c59cd897c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quant_config_ptq(bits):\n",
    "    q_config = torch.ao.quantization.QConfig(\n",
    "        activation=quantize.MinMaxObserver.with_args(\n",
    "                            quant_min=...,\n",
    "                            quant_max=...,\n",
    "                            dtype=torch.quint8),\n",
    "        weight=quantize.MinMaxObserver.with_args(\n",
    "                            quant_min=...,\n",
    "                            quant_max=...,\n",
    "                            dtype=torch.qint8),\n",
    "    ) \n",
    "    return q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b16444-0e1e-4402-8da9-23d2a1d6a57c",
   "metadata": {},
   "source": [
    "Then, we can instantiate a ``QConfig`` object for the desired bit-width, which we add to the floating-point model. We use ``prepare`` to put the floating-point model into calibration mode, and we provide a representative dataset (here we provide the entire training dataset) that PyTorch uses to calculate activation statistics since these are only known at run-time. Finally, we use ``convert`` to create the quantized model and we evaluate its accuracy on the calibration data for comparison. You will observe that with 8-bit quantization the accuracy loss is very small, but with smaller bit-widths the loss starts becoming more significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d1eaa-e9d9-409d-ad76-5de0e090958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration and preparation\n",
    "bits = 8\n",
    "model_fp.qconfig = get_quant_config_ptq(bits)\n",
    "model_fp.eval() # Model needs to be in evaluation mode to collect statistics\n",
    "model_fp_prepared_ptq = quantize.prepare(model_fp)\n",
    "\n",
    "# Provide representative dataset to collect statistics\n",
    "for _, (imgs, labels) in enumerate(train_loader):    \n",
    "    out = model_fp_prepared_ptq(imgs)\n",
    "\n",
    "# Quantize model and run it to obtain quantized accuracy\n",
    "model_ptq = quantize.convert(model_fp_prepared_ptq)    \n",
    "accuracy_ptq = accuracy(model_ptq,test_loader)\n",
    "\n",
    "print(f\"Your floating-point accuracy is {accuracy_fp:.3f} and quantized accuracy is {accuracy_ptq:.3f}\")\n",
    "print(f\"Indicative floating-point accuracy is 0.975 and quantized accuracy is 0.961\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30499e3f-f1c7-439f-8472-f7d96dc68c7e",
   "metadata": {},
   "source": [
    "### 2.2 Quantization-aware training (QAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d5990-c459-410e-9820-5976d0a11167",
   "metadata": {},
   "source": [
    "Similarly to PTQ, we need to define the quantization parameters using a ``QConfig`` object. The main difference is that we use a ``FakeQuantize`` module that simulates quantization during the training, while also keeping track of statistics with an observer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911452c-efb6-4499-a988-ac7bc001760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quant_config_qat(bits):\n",
    "    q_config = torch.ao.quantization.QConfig(\n",
    "        activation=quantize.FakeQuantize.with_args(\n",
    "                            observer=quantize.observer.MinMaxObserver,\n",
    "                            quant_min=...,\n",
    "                            quant_max=...,\n",
    "                            dtype=torch.quint8),\n",
    "        weight=quantize.FakeQuantize.with_args(\n",
    "                            observer=quantize.observer.MinMaxObserver,\n",
    "                            quant_min=...,\n",
    "                            quant_max=...,\n",
    "                            dtype=torch.qint8),\n",
    "    ) \n",
    "    return q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d10e58-f1c0-465d-a273-6433fbae4a27",
   "metadata": {},
   "source": [
    "Then, we can instantiate a ``QConfig`` object for the desired bit-width, which we add to the floating-point model. We use ``prepare_qat`` to put the floating-point model into quantization-aware training mode (note that the floating-point model needs to be in training mode). We then define the training hyperparameters as usual and run one epoch of post-training quantization (remember that the model has already been trained for five epochs previously). Finally, we quantize the model and we calculate its accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bd0f8-88ba-4abc-bf17-16fc037980d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model_fp.train()\n",
    "bits = 8\n",
    "model_fp.qconfig = get_quant_config_qat(bits)\n",
    "model_fp_prepared_qat = quantize.prepare_qat(model_fp)\n",
    "\n",
    "# Define training hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_fp_prepared_qat.parameters(), lr=0.002)\n",
    "batch_size = 128\n",
    "num_epochs = 1\n",
    "log=False\n",
    "\n",
    "# Run quantization-aware training\n",
    "train(model_fp_prepared_qat,train_loader,test_loader,criterion,optimizer,batch_size,num_epochs,log)\n",
    "\n",
    "# Quantize model and run it to obtain quantized accuracy\n",
    "model_fp_prepared_qat.eval()\n",
    "model_qat = quantize.convert(model_fp_prepared_qat)    \n",
    "accuracy_qat = accuracy(model_qat,test_loader)\n",
    "\n",
    "print(f\"Your floating-point accuracy is {accuracy_fp:.3f} and quantized accuracy is {accuracy_qat:.3f}\")\n",
    "print(f\"Indicative floating-point accuracy is 0.975 and quantized accuracy is 0.966\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ddc71-ec22-49d1-a8b5-fc36d353dcab",
   "metadata": {},
   "source": [
    "### 2.3 Comparison of PTQ and QAT\n",
    "\n",
    "The difference in accuracy between PTQ and QAT in the above example is very small, although QAT still has an edge. This happens because we use 8 quantization bits, which, as you will see, is significantly more than what is required for this simple CNN. To illustrate this point, below we implement a loop that explores quantization bit-widths from 2 to 8 bits. For each run of the loop, a new quantization configuration is instantiated for PTQ and QAT, then the floating-point model is prepared using that configuration, a representative dataset is fed into the model (for PTQ) or a training iteration is performed (for QAT), and finally the corresponding models are quantized and their accuracy is evaluated on the test set and stored in a list for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbab62-6fda-4e47-8009-49c743a929fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "accuracy_ptq_list = list()\n",
    "accuracy_qat_list = list()\n",
    "bits_list = range(2,9)\n",
    "\n",
    "# Training hyperparameters for QAT\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "num_epochs = 1\n",
    "log=False\n",
    "\n",
    "for bits in bits_list:\n",
    "\n",
    "    ### PTQ\n",
    "    \n",
    "    # Configure, provide representative dataset, and quantize model (multiple lines of code required)\n",
    "    ...\n",
    "    model_ptq = ...\n",
    "    accuracy_ptq_list.append(accuracy(model_ptq,test_loader))\n",
    "\n",
    "    ### QAT\n",
    "\n",
    "    # Configure, train, and quantize model (multiple lines of code required)\n",
    "    ...\n",
    "    model_qat = ...\n",
    "    accuracy_qat_list.append(accuracy(model_qat,test_loader))\n",
    "\n",
    "# Print results\n",
    "print('Your PTQ accuracies: [' + ' '.join('{:.3f}'.format(acc) for i,acc in enumerate(accuracy_ptq_list)) + ']')\n",
    "print('Your QAT accuracies: [' + ' '.join('{:.3f}'.format(acc) for i,acc in enumerate(accuracy_qat_list)) + ']')\n",
    "print('Indicative PTQ accuracies: [0.138 0.655 0.937 0.969 0.973 0.974 0.961]')\n",
    "print('Indicative QAT accuracies: [0.247 0.828 0.960 0.969 0.976 0.974 0.959]')\n",
    "\n",
    "# Plot results\n",
    "plt.plot(bits_list,100*accuracy_fp*np.ones(len(accuracy_ptq_list)))\n",
    "plt.plot(bits_list,100*np.array(accuracy_ptq_list), 'o-')\n",
    "plt.plot(bits_list,100*np.array(accuracy_qat_list), 's-')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Quantization Bits')\n",
    "plt.legend(['Floating-point','Post-Training Quantization','Quantization-Aware Training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd234b1-678a-44ca-a658-cfeeaa9949e3",
   "metadata": {},
   "source": [
    "## 3. Pruning\n",
    "\n",
    "PyTorch supports various types of pruning. For example, it is possible to prune each layer separately and there are various pruning methods (e.g., structured/unstructured and based on various criteria like the $L_1$-norm or the $L_2$-norm). Not all combinations are natively supported, but it is possible to write your own custom pruning method. We will use simple, albeit very effective, global pruning (i.e., all layers are pruned jointly). More specifically, the variable ``sparsity`` controls the desired sparsity factor and the individual weights with the smallest $L_1$ norm (i.e., the smallest absolute value) are removed from the model. In this example, we use ``sparsity=0.75``, which means that only 25% of the weights are kept in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89613fab-9465-4f3a-bdb4-2f73b941669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-instantiate CNN\n",
    "out_chans = 16\n",
    "filter_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "maxpool_size = 2        \n",
    "model_pruned = MNISTCNN(out_chans,filter_size,stride,padding,maxpool_size,quant_flag=False)\n",
    "\n",
    "# Copy weights from previously trained model\n",
    "model_pruned.load_state_dict(model.state_dict())\n",
    "\n",
    "# Calculate accuracy on test set before pruning\n",
    "accuracy_unpruned = accuracy(model,test_loader)\n",
    "\n",
    "# Create a list of parameters to prune\n",
    "parameters_to_prune = ( (model_pruned.conv, 'weight'), (model_pruned.fc, 'weight') )\n",
    "\n",
    "# Apply pruning\n",
    "sparsity=0.75\n",
    "prune.global_unstructured(parameters_to_prune,pruning_method=prune.L1Unstructured,amount=sparsity)\n",
    "\n",
    "# Calculate per-layer and global sparsity\n",
    "print(\"Sparsity in convolutional layer: {:.2f}%\".format(100. * float(torch.sum(model_pruned.conv.weight == 0))/float(model_pruned.conv.weight.nelement())))\n",
    "print(\"Sparsity in fully connected layer {:.2f}%\".format(100. * float(torch.sum(model_pruned.fc.weight == 0))/float(model_pruned.fc.weight.nelement())))\n",
    "print(\"Global sparsity: {:.2f}%\".format(100. * float(torch.sum(model_pruned.conv.weight == 0)+torch.sum(model_pruned.fc.weight == 0))/float(model_pruned.conv.weight.nelement()+model_pruned.fc.weight.nelement())))\n",
    "\n",
    "# Calculate accuracy on test set after pruning\n",
    "accuracy_pruned = accuracy(model_pruned,test_loader)\n",
    "\n",
    "print(\"The accuracy before pruning is {:.3f} and the accuracy after pruning is {:.3f}\".format(accuracy_unpruned, accuracy_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4be1d2-6a62-49ed-84c0-2ea90bf29b00",
   "metadata": {},
   "source": [
    "### 3.1 Fine-tuning\n",
    "\n",
    "When pruning 75% of the weights, the accuracy dropped quite significantly to around 84%. Even though the model is still usable with that accuracy, we would ideally like to recover the lost accuracy. This can be achieved by fine-tuning the pruned model through additional training.\n",
    "\n",
    "When a layer is pruned in PyTorch, its original ``weight`` parameter is replaced by two new parameters: ``weight_orig``, which contains the original weights before pruning, and ``weight_mask``, which contains a binary pruning mask that is point-wise multiplied with ``weight_orig`` to obtain the weights of the pruned model (i.e., the mask forces pruned weights to zero). This is convenient because we can fine-tune the model by simply running our training function, which will update the weights in ``weight_orig``, and ``weight_mask`` will ensure that the sparsity factor is unchanged. This additional information added for pruning can be removed from a layer with the ``prune.remove()`` method, but then fine-tuning will update all weights including previously pruned ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc795bd-651b-434b-a0d5-7ae9b70acbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# Define training hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_pruned.parameters(), lr=0.002)\n",
    "batch_size = 128\n",
    "num_epochs = 1\n",
    "log=False\n",
    "\n",
    "# Fine-tune\n",
    "train(model_pruned,train_loader,test_loader,criterion,optimizer,batch_size,num_epochs,log)\n",
    "\n",
    "# Calculate accuracy on test set after fine-tuning\n",
    "accuracy_pruned_finetuned = accuracy(model_pruned,test_loader)\n",
    "\n",
    "# Print accuracies and confirm that sparsity is unchanged after training\n",
    "print(\"Global sparsity: {:.2f}%\".format(100. * float(torch.sum(model_pruned.conv.weight == 0)+torch.sum(model_pruned.fc.weight == 0))/float(model_pruned.conv.weight.nelement()+model_pruned.fc.weight.nelement())))\n",
    "print(\"The accuracy before pruning is {:.3f}, the accuracy after pruning is {:.3f}, the accuracy after pruning and fine-tuning is {:.3f}\".format(accuracy_unpruned, accuracy_pruned, accuracy_pruned_finetuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0073aab-f5b4-484e-8d5d-8a1e4c7b6212",
   "metadata": {},
   "source": [
    "### 3.2 Sparsity ratio exploration\n",
    "\n",
    "To show the effect of the sparsity ratio, we now prune and evaluate the accuracy of the pruned model for various sparsity ratios (both with and without fine-tuning). The fine-tuned pruned model supports a sparsity ratio of up to 0.75 without any degradation in the accuracy, which means that 75% of the weights are unnecessary. Further pruning is possible at the cost of slightly reduced accuracy, and the CNN still achieves approximately 90% accuracy even with a staggering 0.95 sparsity ratio, i.e., with only 5% of the original weights! Without fine-tuning, the degradation is much more rapid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15333fb1-6c0e-49a4-a181-be5b77ad9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "sparsity_list = [x / 100.0 for x in range(50, 100, 5)]\n",
    "accuracy_pruned_list = list()\n",
    "accuracy_pruned_finetuned_list = list()\n",
    "\n",
    "for sparsity in sparsity_list:\n",
    "\n",
    "    # Define model\n",
    "    ...\n",
    "\n",
    "    # Prune model\n",
    "    ...\n",
    "\n",
    "    # Fine-tune model\n",
    "    ...\n",
    "\n",
    "# Print results\n",
    "print('Your pruning accuracies: [' + ' '.join('{:.3f}'.format(acc) for i,acc in enumerate(accuracy_pruned_list)) + ']')\n",
    "print('Your pruning+finetuning accuracies: [' + ' '.join('{:.3f}'.format(acc) for i,acc in enumerate(accuracy_pruned_finetuned_list)) + ']')\n",
    "print('Indicative pruning accuracies: [0.960 0.960 0.940 0.909 0.892 0.908 0.718 0.535 0.399 0.424]')\n",
    "print('Indicative pruning+finetuning accuracies: [0.976 0.974 0.974 0.974 0.973 0.972 0.966 0.958 0.935 0.823]')\n",
    "\n",
    "# Plot results\n",
    "plt.plot(sparsity_list,100*accuracy_unpruned*np.ones(len(sparsity_list)))\n",
    "plt.plot(sparsity_list,100*np.array(accuracy_pruned_list), 'o-')\n",
    "plt.plot(sparsity_list,100*np.array(accuracy_pruned_finetuned_list), 's-')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Sparsity Ratio')\n",
    "plt.legend(['Unpruned','Pruned','Pruned and Fine-Tuned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca4bb2-ff6c-4694-a38e-1462b1639a42",
   "metadata": {},
   "source": [
    "## 4. Combining Quantization and Pruning\n",
    "\n",
    "Quantization and pruning can be combined. In the example below, we first prune our model with fine-tuning and we then quantize it using PTQ. Note that we have to use ``prune.remove`` discussed previously before quantization to remove the pruning re-parameterization as otherwise quantization will not work. Here, we choose ``sparsity=0.75`` and ``bits=6`` because these were the values that we previously saw resulted in practically no loss with respect to the un-optimized CNN when only pruning and only applying PTQ, respectively. The resulting pruned and quantized CNN also shows a very minimal accuracy loss with respect to the unoptimized CNN. In general, however, there is a complex interaction between pruning and quantization, meaning that the pruning ratio and the quantization bit-width cannot usually be optimized separately. In this script, we also calculate the average execution time for one inference for the unoptimized CNN, the pruned CNN, and the pruned and quantized CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffad56-f275-421a-92f6-ba10d8a72c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.manual_seed(0)\n",
    "\n",
    "## Define model\n",
    "model_pruned = MNISTCNN(out_chans,filter_size,stride,padding,maxpool_size,quant_flag=True)\n",
    "model_pruned.load_state_dict(model.state_dict())\n",
    "\n",
    "# Get accuracy and execution time of unoptimized model\n",
    "start_unoptimized = timeit.default_timer()\n",
    "accuracy_unoptimized = accuracy(model,test_loader)\n",
    "end_unoptimized = timeit.default_timer()\n",
    "runtime_unoptimized = (end_unoptimized-start_unoptimized)/len(test_loader)\n",
    "\n",
    "## Prune floating-point model\n",
    "\n",
    "# Prune\n",
    "sparsity = 0.75\n",
    "parameters_to_prune = ( (model_pruned.conv, 'weight'), (model_pruned.fc, 'weight') )\n",
    "prune.global_unstructured(parameters_to_prune,pruning_method=prune.L1Unstructured,amount=sparsity)\n",
    "\n",
    "# Fine-tune\n",
    "optimizer = torch.optim.Adam(model_pruned.parameters(), lr=0.002)\n",
    "train(model_pruned,train_loader,test_loader,criterion,optimizer,batch_size,num_epochs,log)                                \n",
    "accuracy_pruned_finetuned = accuracy(model_pruned,test_loader)\n",
    "\n",
    "# Remove re-parameterization\n",
    "prune.remove(model_pruned.conv, name='weight')\n",
    "prune.remove(model_pruned.fc, name='weight')\n",
    "\n",
    "# Get accuracy and execution time of pruned model\n",
    "start_pruned = timeit.default_timer()\n",
    "accuracy_pruned = accuracy(model_pruned,test_loader)\n",
    "end_pruned = timeit.default_timer()\n",
    "runtime_pruned = (end_pruned-start_pruned)/len(test_loader)\n",
    "\n",
    "## Quantize pruned model with PTQ\n",
    "\n",
    "# Apply configuration\n",
    "bits = 6\n",
    "model_pruned.qconfig = get_quant_config_ptq(bits)\n",
    "model_pruned_prepared_ptq = quantize.prepare(model_pruned)\n",
    "\n",
    "# Provide representative dataset to collect statistics\n",
    "for _, (imgs, labels) in enumerate(train_loader):    \n",
    "    out = model_pruned_prepared_ptq(imgs)\n",
    "\n",
    "# Quantize\n",
    "model_pruned_ptq = quantize.convert(model_pruned_prepared_ptq) \n",
    "\n",
    "# Get accuracy and execution time of pruned and quantized model\n",
    "start_pruned_ptq = timeit.default_timer()\n",
    "accuracy_pruned_ptq = accuracy(model_pruned_ptq,test_loader)\n",
    "end_pruned_ptq = timeit.default_timer()\n",
    "runtime_pruned_ptq = (end_pruned_ptq-start_pruned_ptq)/len(test_loader)\n",
    "\n",
    "print(\"Accuracy: unoptimized {:.3f}, pruned {:.3f}, pruned and quantized {:.3f}\".format(accuracy_fp,accuracy_pruned, accuracy_pruned_ptq))\n",
    "print(\"Average per-inference runtime: unoptimized {:.2f} ms, pruned {:.2f} ms, pruned and quantized {:.2f} ms\".format(runtime_unoptimized*10**3,runtime_pruned*10**3, runtime_pruned_ptq*10**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21da01b-eee1-43ac-b3a0-239af6088464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
