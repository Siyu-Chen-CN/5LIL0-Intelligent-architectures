{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b630e4",
   "metadata": {},
   "source": [
    "# Intelligent Architectures (5LIL0) Assignment 1 (version 0.3)\n",
    "\n",
    "#### Authors: Marzieh Hashemipour-Nazari, Alexios Balatsoukas-Stimming, License: [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
    "\n",
    "## Part A: Multi-layer perceptron for handwritten digit recognition from scratch\n",
    "\n",
    "In the first part of lab 1, we want to generate a multilayer perceptron for handwritten digit classification from scratch: Using only  `NumPy` and `matplotlib` libraries.\n",
    "\n",
    "## Import\n",
    "Let us import these two libraries. We also set the number of threads for various libraries to one for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fce87",
   "metadata": {},
   "source": [
    "## 1. Data preparation \n",
    "Our data is from the **MNIST** dataset of 28x28 pixel images of handwritten digits from 0-9. We first need to unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7537fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip data\n",
    "!cd data && tar xzf mnist_data.tar.gz && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80db69",
   "metadata": {},
   "source": [
    "### 1.1 Load the data\n",
    "After unzipping, we have two comma-separated values (csv) files: **mnist_train.csv** and **mnist_test.csv** for training and testing, respectively. First, we need to load our data. Let's do it by defining a new function called `LoadData`. \n",
    "\n",
    "We have 60000 samples in the train set and 10000 samples in the test set. Each sample, which is a 28 x 28 image, is unrolled into a 1-dimensional vector of length 28 x 28 = 784. The `LoadData()` function loads the data into two separate arrays $X$ and $Y$. We can determine the number of samples we want for training and testing our network with the `samples` input value. The upper bound for `samples` for the training and test data is 60000 and 10000, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af696d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(DirName,samples = 500):\n",
    "    data = list();\n",
    "    count = 0;\n",
    "    with open(DirName) as f:\n",
    "        for line in f:\n",
    "            if count<samples:\n",
    "                new_line = np.array(line.split(','))\n",
    "                new_line = new_line.astype(np.float32)\n",
    "                data.append(new_line)\n",
    "                count += 1\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    return data[:,0],data[:,1:]\n",
    "\n",
    "NoTrain = 60000\n",
    "NoTest = 10000\n",
    "Y_train,x_train = LoadData(\"./data/mnist_train.csv\", NoTrain)\n",
    "Y_test,x_test = LoadData(\"./data/mnist_test.csv\", NoTest)\n",
    "\n",
    "print(f\"Test set size: {x_test.shape[0]} x {x_test.shape[1]}\")\n",
    "print(f\"Train set size: {x_train.shape[0]} x {x_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a67fde",
   "metadata": {},
   "source": [
    "### 1.2 MNIST visualization\n",
    "Here, we just want to show how the dataset looks like with a sample example defined by the `sample` variable. To do this, we need to resize the flattened length-784 representation into a 28 x 28 matrix for compatibility with the `imshow()` function from `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea05f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = 7;  # index of the image we want to show\n",
    "\n",
    "plt.title(f'The handwritten digit is {Y_train[sample]:.0f}')\n",
    "\n",
    "# Reshape the array into 28 x 28 array (2-dimensional array)\n",
    "pixels = x_train[sample,:].reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray',vmin=0,vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46135e52",
   "metadata": {},
   "source": [
    "### 1.3 Rescaling and encoding the data\n",
    "#### Rescaling feature values (input data) \n",
    "The input data in the $X$ array takes on values from 0 to 255. Rescaling the data to a smaller range can help with training convergence. One common way is the standardization method where the data is rescaled so that its mean is 0 and its standard deviation is 1. This is done using the following formula:\n",
    "$$\n",
    "X^{\\prime}=\\frac{X-\\mu}{\\sigma},\n",
    "$$\n",
    "where $\\mu$ is the mean of the training data features and $\\sigma$ is the standard deviation of the training data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize training and test data\n",
    "x_train_st = ...\n",
    "x_test_st = ...\n",
    "\n",
    "print(f\"Your training data mean is {np.mean(x_train_st):.3f} and your standard deviation is {np.std(x_train_st):.3f}\")\n",
    "print(f\"Correct training data mean is {-0.000:.3f} and correct standard deviation is {1.000:.3f}\")\n",
    "print(f\"Your test data mean is {np.mean(x_test_st):.3f} and your standard deviation is {np.std(x_test_st):.3f}\")\n",
    "print(f\"Correct test data mean is {0.006:.3f} and correct standard deviation is {1.008:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae72bfc",
   "metadata": {},
   "source": [
    "#### Encoding the output \n",
    "In addition, for categorical data (where image is labeled between 0-9), we need the one-hot encoded representation of our data, meaning each label is converted into a binary vector (e.g. the label 9 would be the vector [0,0,0,0,0,0,0,0,0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding train and test sets labels \n",
    "y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_train[np.arange(Y_train.size),Y_train.astype(int)] = 1.0;\n",
    "\n",
    "y_test  = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n",
    "y_test[np.arange(Y_test.size),Y_test.astype(int)] = 1.0;\n",
    "\n",
    "print(f\"Your decimal label is {Y_train[0]:.0f} and your one-hot encoded label is {y_train[0,:]}\")\n",
    "print(f\"Correct decimal label is 5 and correct one-hot encoded label is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef35382-eeb5-492b-ae2d-99d5fa6259f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Neural networks (NN)\n",
    "Neural networks (NN) are a collection of connected layers consisting of nodes called artificial neurons. Each connection transmits a \"signal\" to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n",
    "The \"signal\" at a connection is a real number, and the output of each neuron is computed by an activation function applied to a weighted sum of its inputs. Signals traverse from the first layer (the input layer), to the last layer (the output layer). \n",
    "The neurons in the hidden layers, which are located between the input and output layers, receive input from the neurons in the input layer or from neurons in earlier hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd8333-eafb-450d-906c-fc10823e7e6e",
   "metadata": {},
   "source": [
    "### 2.1 Multi-layer perceptrons (MLP)\n",
    "The most well-known NN model is the multi-layer perceptron (MLP), which is a feed-forward fully-connected network. \n",
    "An MLP model has fully-connected layers such that every input neuron is connected to every neuron in the next layer, where information only moves forward (no feedback). Therefore, neurons move the input value of one layer to the next layer. Mathematically, the MLP model defines a parameterized function in which each layer computes a weighted linear combination, $h =XW+b$ of the layer input denoted by the vector $X$, followed by an activation function $\\sigma(h)$. MLP configuration can be repeatedly chained to build a deep neural network (DNN).\n",
    "The following figure shows the structure of a simple MLP with one hidden layer.\n",
    "\n",
    "<img src=\"mlp.jpg\" width=\"600\">\n",
    "\n",
    "Image credit: [https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron](https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron)\n",
    "\n",
    "$W_i$ denotes the weights connecting the $(i âˆ’ 1)$-th layer to the $i$-th layer and determines the strength and sign of the connection between these two layers. In addition to the weights, there is usually a bias term (denoted by $b$) to adjust the offset. The output of the $i$-th layer, denoted by $O^i$, is determined as:\n",
    "$$\n",
    "O^{(i)} = \\sigma\\left(O^{(i-1)}W^{(i)}+b^{(i)}\\right),\n",
    "$$\n",
    "where $\\sigma(x)$ is the activation function for the corresponding layer. The output of the input layer $O^{(0)}$ is the input vector $X$. In our example, the output layer has 10 dimensions for representing the one-hot encoded binary vector of output labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c327fb-950e-43fc-ac96-00184b785880",
   "metadata": {},
   "source": [
    "### 2.2 Activation functions\n",
    "The activation function depicted as $\\sigma(\\cdot)$ is an element-wise non-linear function. There are multiple activation functions available for different NN models. In this lab, we use two of them called **sigmoid** and **ReLU** for the hidden layer and output layer, respectively.\n",
    "\n",
    "__sigmoid:__$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "__ReLU:__ $$ \\sigma(x)= \\begin{cases}0, &  x<0, \\\\ x, &  x\\geq 0.\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b0206",
   "metadata": {},
   "source": [
    "### 2.3 Forward pass\n",
    "\n",
    "The process of generating an output from an input is called a forward pass. For an MLP, the forward pass is basically a series of matrix multiplications followed by activation function applications. We want to define an MLP with one hidden layer for the digit recognition problem with MNIST dataset. Below is the figure for the desired MLP model:\n",
    "\n",
    "<img src=\"nn_mnist.jpg\" width=\"600\">\n",
    "\n",
    "__Note:__ The superscript $i={0,1,2}$ in the figure indicates the associated layer number. In addition, for the sake of simplicity, the required calculations and output only for the first neuron in each layer are specified in the figure. The remaining neurons in each layer have the same structure as the first one.\n",
    "\n",
    "There are 128 neurons for the hidden layer. As said before, the activation function for the hidden layer is different from the output layer. \n",
    "As shown in the figure, the output of each neuron in the hidden layer will be calculated as follows:\n",
    "$$\n",
    "O^{(1)}_{j}=\\text{ReLU}\\left(\\sum_{i=0}^{783} x_i \\times w^{(0)}_{i, j}\\right) \\; \\text{ for } \\; j={1,2,...,128}\n",
    "$$\n",
    "(__note:__ There is no bias term in this example). \n",
    "As the figure shows, the weights $w^{(0)}_{i,j}$ need to be compacted into a matrix $W^{(0)}\\in \\mathbb{R}^{784\\times128}$, allowing for all the neurons in the hidden layer to be efficiently calculated using matrix multiplication. As a different weight matrix is needed to transform the hidden layer to the output layer, $W^{(0)}$ is the matrix that transforms the inputs to the hidden layer:\n",
    "$$\n",
    "O^{(1)} = \\text{ReLU}\\left(h^{(0)}\\right), \\text{ where } h^{(0)}=XW^{(0)}  \n",
    "$$\n",
    "Similarly, $W^{(1)}$ transforms the hidden layer to the output layer as follows:\n",
    "$$\n",
    "O^{(2)}_{j}=\\text{sigmoid}\\left(O^{(1)}W^{(1)}\\right), O^{(1)} \\in \\mathbb{R}^{1\\times 128}, W^{(1)} \\in \\mathbb{R}^{128\\times 10} \n",
    ".$$\n",
    "\n",
    "The last step of the classification task is producing a prediction from $O^{(2)}_j, j=\\{0,1,...,M-1\\}$ in the output layer, where $M$ is the number of labels ($M=10$ in the MNIST dataset). Typically, for the multi-label classification, the **softmax** function is used to convert the output of the $M$ neurons in the output layer into probabilities as follows:\n",
    "$$\n",
    "\\text{softmax}(z_j) =\\frac{e^{z_j}}{\\sum^{M-1}_{i=0}e^{z_i}},\n",
    "$$\n",
    "The interesting property of this function is that the sum of all the outputs of softmax, is always equal to 1:\n",
    "$$\n",
    "\\sum^{M-1}_{i=0}\\text{softmax}({z_i}) = 1\n",
    "$$\n",
    "Therefore, applying softmax to the output layer of our example effectively converts the output of neurons into probabilities for each label in the vector $\\hat{y}$ as follows:\n",
    "$$\n",
    "\\hat{y}_j = \\text{softmax}\\left({O^{(2)}_j}\\right), \\; j\\in \\{0,1,...,M-1\\}.\n",
    "$$\n",
    "\n",
    "Now, we have revisited the math behind the forward pass for an MLP model. So, let's start implementing everything we have looked at in the python code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fcd3e",
   "metadata": {},
   "source": [
    "### 3. MLP Implementation\n",
    "\n",
    "#### 3.1 Input initialization\n",
    "As we want to write efficient and fast code, we use matrix (NumPy multidimensional arrays) operations whenever possible. NumPy will automatically use parallelism whenever possible, which will help to substantially accelerate our code (this is also sometimes called vectorizing your code).\n",
    "\n",
    "The input vector $X$, weight matrices $W^{(0)}$ and $W^{(1)}$, and activation functions are all we need to define the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc373281",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train_st \n",
    "Y = y_train \n",
    "\n",
    "N_l = 128  # number of neurons in hidden layer\n",
    "\n",
    "layers = np.array([X.shape[1]]+[N_l]+[Y.shape[1]]) # layers = [784 128 10]\n",
    "\n",
    "print('Your', '\\033[1m' + 'layers' + '\\033[0m','array is:', layers)\n",
    "print('Correct ', '\\033[1m' + 'layers' + '\\033[0m','array is: [784 128  10]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1daad0",
   "metadata": {},
   "source": [
    "#### 3.2 Weight initialization\n",
    "In order to perform a forward pass, our input vector $X$ is consecutively multiplied by weight matrices and then passed into the corresponding activation functions.\n",
    "The values of the weight matrices will ultimately be learned through backpropagation, but each weight matrix must first be initialized to random values.\n",
    "There are several different methods for doing this initialization. Here, we use a simple approach to initiate the values from a uniform distribution on the range [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0591f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for feedforward\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "def init_weights(layers):\n",
    "    weights = list()\n",
    "    # for hidden layer\n",
    "    weights_l1 = ...  # weights for hidden layer\n",
    "    weights.append(weights_l1)\n",
    "    #for output layer\n",
    "    weights_l2 = ... # weights for output layer\n",
    "    weights.append(weights_l2)\n",
    "    \n",
    "    weights = np.asarray(weights,dtype=object)\n",
    "    return weights\n",
    "\n",
    "weights = init_weights(layers)\n",
    "\n",
    "print(f\"Dimension of your weight matrix for the hidden layer is {weights[0].shape[0]} x {weights[0].shape[1]} and for the output layer is {weights[1].shape[0]} x {weights[1].shape[1]}\")\n",
    "print(\"Dimension of the correct weight matrix for the hidden layer is 784 x 128 and for the output layer is 128 x 10\")\n",
    "\n",
    "print(f\"Your initialized weight connecting the first neuron in the hidden layer to the first neuron in the output layer is {(weights[1][0][0]):.6f}\")\n",
    "print(\"Correct initialized weight connecting the first neuron in the hidden layer to the first neuron in the output layer is 0.705750\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7cdd6",
   "metadata": {},
   "source": [
    "#### 3.3 Activation functions\n",
    "\n",
    "Here, we will define the activation functions required by our network, namely the `ReLU()`, `sigmoid()`, and `softmax()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Input Parameters:\n",
    "    x : input: float of array\n",
    "    \n",
    "    Returns:\n",
    "    ReLU(x) : float or array\n",
    "    \"\"\"\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Input Parameters:\n",
    "    x : input: float of array\n",
    "    \n",
    "    Returns:\n",
    "    sigmoid(x) : float or array\n",
    "    \"\"\"\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "def softmax(x):  \n",
    "    \"\"\"\n",
    "    Input Parameters:\n",
    "    x : input: array(n x p) : n samples by p dimensions : p=10 for MNIST (because we have 0-9 digits)\n",
    "    \n",
    "    Returns:\n",
    "    softmax(x) : float or array\n",
    "    \"\"\"\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "print(f\"Your outputs: ReLU(-0.1) = {ReLU(-0.1):.3f}, ReLU(0.1) = {ReLU(0.1):.3f}, sigmoid(-0.1) = {sigmoid(-0.1):.3f}, sigmoid(0.1) = {sigmoid(0.1):.3f}, softmax([0.1 0.2] =\", softmax([[0.1, 0.2]]))\n",
    "print(\"Correct outputs: ReLU(-0.1) = 0.000, ReLU(0.1) = 0.100, sigmoid(-0.1) = 0.475, sigmoid(0.1) = 0.525, softmax([0.1 0.2] = [[0.47502081 0.52497919]]) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a6491-db11-4c57-8ca9-83fbc5ce8ee7",
   "metadata": {},
   "source": [
    "### 3.4 Layer initialization\n",
    "The values for hidden and output layers will be stored in multi-dimensional matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e3920-c8b5-42d0-bd57-ff7f42ed5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the layer outputs\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "def init_layer(layers,n_samples):\n",
    "    h = [np.empty((n_samples,layers[1])),np.empty((n_samples,layers[2]))] \n",
    "    o = [np.empty((n_samples,layers[0])),np.empty((n_samples,layers[1])),np.empty((n_samples,layers[2]))] \n",
    "    return h,o\n",
    "\n",
    "h,o = init_layer(layers,n_samples)\n",
    "\n",
    "print(f\"h size (hidden layer): {h[0].shape[0]} x {h[0].shape[1]}\")\n",
    "print(f\"h size (output layer): {h[1].shape[0]} x {h[1].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e31d6",
   "metadata": {},
   "source": [
    "### 3.5 Forward pass\n",
    "For simplicity, we ask you to implement a hard-coded version of the specific MLP, i.e., your code does not need to be generic for any number of layers, neurons, etc. Therefore, you only need to implement the matrix multiplication and apply the requested activation functions for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(data,weights,o,h):\n",
    "    \n",
    "    # Input layer\n",
    "    o[0] = data\n",
    "    \n",
    "    # Hidden layer --> ReLU activation\n",
    "    h[0] = ...\n",
    "    o[1] = ...\n",
    "    \n",
    "    # Output Layer --> sigmoid + softmax activations\n",
    "    h[1] = ...\n",
    "    o[2] = ...\n",
    "    Y_hat = softmax(o[2]) \n",
    "    \n",
    "    return Y_hat\n",
    "\n",
    "Y_hat = feedforward(X,weights,o,h)\n",
    "\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print('Your predicted output for the first training sample is:',Y_hat[0])\n",
    "print('Correct predicted output for the first training sample is: [0.0593 0.1611 0.0593 0.1611 0.0593 0.0593 0.1611 0.0593 0.1611 0.0593]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f599324",
   "metadata": {},
   "source": [
    "## 4. Loss/Cost function\n",
    "A loss function, which quantifies how \"wrong\" the neural network's output $\\hat{\\mathbf{y}}$ is compared to the expected output $\\mathbf{y}$, is used to assess the neural network's performance.\n",
    "### 4.1 Cross-entropy loss function\n",
    "The loss function should reflect the objective of the defined neural network. For example, in our design, we want to classify the handwritten digits from 0-9. The cross-entropy loss, also known as the negative log-likelihood, is most commonly used for classification problems. Below is the mathematical representation of this function:\n",
    "$$\n",
    "\\text{L}(\\mathbf{y},\\hat{\\mathbf{y}}) = -\\sum_{k=0}^{M-1}{y_k \\log(\\hat{y}_k)},\n",
    "$$\n",
    "where $M$ is the number of distinct labels. The following equation is used to define the cost function over $N$ input samples:\n",
    "$$\n",
    "\\text{C} = \\sum_{j=1}^{N}{\\text{L}(\\mathbf{y}^{(i)},\\hat{\\mathbf{y}}^{(i)})}.\n",
    "$$\n",
    "The following function calculates the overall cost function $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f772ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy loss  \n",
    "def cross_entropy(y_pred,y):\n",
    "    \"\"\"\n",
    "    Input Parameters:\n",
    "    y_pred, y : array of float\n",
    "    \n",
    "    Returns:\n",
    "    c : float\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute loss\n",
    "    c = ...\n",
    "    return c\n",
    "\n",
    "loss = cross_entropy(Y_hat,Y)\n",
    "print(f\"Your loss with randomly initialized weights is {loss:.3f}\")\n",
    "print(f\"Correct loss with randomly initialized weights is 143488.412\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66059357",
   "metadata": {},
   "source": [
    "### 4.2 Accuracy \n",
    "The input is normally classified according the output label with the highest likelihood. \n",
    "$$\n",
    "\\text{prediction}(x) = \\text{argmax}(\\hat{\\mathbf{y}}) \n",
    "$$\n",
    "Therefore, to evaluate the classification accuracy of our model, we must identify the output label with the highest probability, assign \"1\" to that label, and set the remaining labels to \"0\". We can then calculate the accuracy of our classifier by comparing the predicted with the actual label in the dataset. The two functions below implement this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1705cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(x):  \n",
    "    # Set the label with the max probability to '1' and the rest to 0 \n",
    "    label = np.zeros((x.shape[0],Y.shape[1]))\n",
    "    label[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "    return label\n",
    "\n",
    "def accuracy(y_pred,y):  \n",
    "    # Calculate the accuracy along the rows, averaging the results over the number of samples.\n",
    "    acc = ...\n",
    "    return acc\n",
    "\n",
    "acc = accuracy(labeling(Y_hat),Y)\n",
    "print(f\"Your accuracy with randomly initialized weights is {acc:.4f}\")\n",
    "print(f\"Correct accuracy with randomly initialized weights is 0.1163\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98b724-ee12-485a-b1ee-332043cfdee6",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "The process of iteratively updating the weights of the network to improve performance is called training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744c810-a384-4b3f-98c7-a168169c37f8",
   "metadata": {},
   "source": [
    "### 5.1 Gradient descent\n",
    "As mentioned earlier, the performance of the neural network is evaluated using the loss function $C$.\n",
    "Therefore, we aim to minimize the loss function by learning the weights of the neural network. This can be done with __gradient descent (GD)__ algorithm defining the update rule as follows:\n",
    "$$\n",
    "W^{(l)}_t = W^{(l)}_{t-1}-\\eta\\frac{\\partial C}{\\partial W^{(l)}_{t-1}},\n",
    "$$\n",
    "where $t$ is the iteration number, $\\eta$ is the learning rate to tune the steps in the GD algorithm, and $\\frac{\\partial C}{\\partial W^{(l)}_{t}}$ is the partial derivative of the loss function with respect to the weight matrix connecting the neurons in layer $(l-1)$ to the neurons in layer $l$. The algorithm takes steps towards the minimum value of a cost function $C$. The size of each step is proportional to the magnitude of the gradient, which is the partial derivative of the cost function with respect to each parameter. \n",
    "\n",
    "Since the derivative of a sum equals a sum of derivatives (i.e., differentiation is a linear operation), we can rewrite the update rule as follows:\n",
    "$$\n",
    "W^{(l)}_{t} = W^{(l)}_{t-1}-\\eta\\sum_{i=1}^{N}\\frac{\\partial L_i}{\\partial W^{(l)}_{t-1}},\n",
    "$$\n",
    "where $L_i=L\\left(\\mathbf{y}^{(i)},\\hat{\\mathbf{y}}^{(i)}\\right)$ is the cross entropy loss for the $i$-th sample in our training set. Once we compute the derivative $\\frac{\\partial L_i}{\\partial W^{(l)}_{t-1}}$ for each training set sample, we can add all individual derivative terms to compute $\\frac{\\partial C}{\\partial W^{(l)}_{t-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6da41",
   "metadata": {},
   "source": [
    "### 5.2 Backpropagation\n",
    "To perform GD for our example, we need to calculate the gradient $\\frac{\\partial C}{\\partial W^{(1)}}$ and $\\frac{\\partial C}{\\partial W^{(0)}}$ for the weight matrices in the output and hidden layers, respectively (note that we drop the subscripts $t$ and $i$ to simplify the notation). Backpropagation is an algorithm to compute these gradients efficiently. To calculate $\\frac{\\partial L}{\\partial W^{(1)} }$, we use the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial h^{(1)}}\\times \\frac{\\partial h^{(1)}}{\\partial W^{(1)}},\n",
    "$$\n",
    "where $\\frac{\\partial h^{(1)}}{\\partial W^{(1)}} = O^{(1)}$ as $h^{(1)}=O^{(1)}W^{(1)}$.\n",
    "For  $\\frac{\\partial L}{\\partial h^{(1)}}$, let's apply the chain rule again and define $\\delta_1$ which we will use later:\n",
    "$$\n",
    "\\delta_1 \\triangleq \\frac{\\partial L}{\\partial h^{(1)}} =  \\frac{\\partial L}{\\partial O^{(2)}} \\times  \\frac{\\partial O^{(2)}}{\\partial h^{(1)}}.\n",
    "$$\n",
    "The term $\\frac{\\partial L}{\\partial O^{(2)}}$ is the partial derivative of the cross entropy loss function with respect to the input of softmax function, and it is equal to $\\hat{y}-y$ (see the detailed explanation [here](https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/)). Since $O^{(2)} = \\text{sigmoid}\\left(h^{(1)}\\right)$, we have $\\frac{\\partial O^{(2)}}{\\partial h^{(1)}} = \\text{sigmoid}\\left(h^{(1)}\\right)\\left(1-\\text{sigmoid}\\left(h^{(1)}\\right)\\right)$, which is the derivative of the sigmoid activation function with respect to its input (see the detailed explanation [here](https://scribe.esmailelbob.xyz/derivative-of-the-sigmoid-function-536880cf918e)). Therefore, putting everything together, we have that:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} = {\\left(O^{(1)} \\right)}^{T} \\left(\\hat{y}-y\\right)\\text{sigmoid}\\left(h^{(1)}\\right)\\left(1-\\text{sigmoid}\\left(h^{(1)}\\right)\\right)\n",
    "$$\n",
    "__Note:__ for the derivative with respect to a matrix, we need to be careful about the matrix dimension. For details, we recommend [this video](https://www.youtube.com/watch?v=GlcnxUlrtek&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&index=5).\n",
    "\n",
    "Next, we need to calculate $\\frac{\\partial L}{\\partial W^{(0)} }$ for the hidden layer in a similar way. Again, we apply the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(0)}} = \\frac{\\partial L}{\\partial h^{(0)}}\\times \\frac{\\partial h^{(0)}}{\\partial W^{(0)}}= {\\left(O^{(0)}\\right)}^{T} \\times \\delta_0\\\\\n",
    "\\delta_0 \\triangleq \\frac{\\partial L}{\\partial O^{(1)}} \\times \\frac{\\partial O^{(1)}}{\\partial h^{(0)}} = \\frac{\\partial L}{\\partial O^{(1)}} \\times \\text{ReLU}'(h^{(0)}),\n",
    "$$\n",
    "where $\\text{ReLU}'(h^{(0)}) = \\begin{cases} 0, & h^{(0)} \\leq 0, \\\\ 1, & h^{(0)} > 1,\\end{cases}$ is the derivative of the ReLU activation function with respect to its input. Continuing the chain rule and using $\\delta_2$ defined previously, we have:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial O^{(1)}} = \\frac{\\partial L}{\\partial h^{(1)}} \\times \\frac{\\partial h^{(1)}}{\\partial O^{(1)}} = \\delta_1 \\times \\left({W^{(1)}}\\right)^{T}.\n",
    "$$\n",
    "So, finally we have:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(0)}} = {\\left(O^{(0)}\\right)}^{T} \\times \\delta_1 \\times {\\left(W^{(1)}\\right)}^{T} \\times \\text{ReLU}'(h^{(0)}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68d3d1-3ee2-45f6-adba-3d01d82cb87b",
   "metadata": {},
   "source": [
    "### 5.3 Backpropagation implementation\n",
    "Now, we want to implement backpropagation for one sample from the dataset. First, we need to implement the derivatives of the **sigmoid** and **ReLU** activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "    derivative = ...\n",
    "    return derivative\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    # Derivative of ReLU function\n",
    "    derivative = ...\n",
    "    return derivative\n",
    "\n",
    "print(f\"Your sigmoid derivative at x = 0.1 is {sigmoid_prime(0.1):.3f}\")\n",
    "print(f\"Correct sigmoid derivative at x = 0.1 is 0.249\")\n",
    "print(f\"Your ReLU derivative at x = 0.1 is {ReLU_prime(0.1):.3f} and at at x = -0.1 is {ReLU_prime(-0.1):.3f}\")\n",
    "print(f\"Correct ReLU derivative at x = 0.1 is 1.000 and at at x = -0.1 is 0.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d36770-81c0-45c8-a964-cddb5aff278b",
   "metadata": {},
   "source": [
    "Then, we calculate $\\delta_1$ and $\\frac{\\partial L}{\\partial W^{(1)}}$ for the output layer and $\\delta_0$ and $\\frac{\\partial L}{\\partial W^{(0)}}$ for the hidden layer, only for the first sample of our training set for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate d(L,w1)\n",
    "delta_1 = ...\n",
    "d_l_w1 = ...\n",
    "\n",
    "# Calculate d(L,w0)\n",
    "delta_0 = ...\n",
    "d_l_w0 = ...\n",
    "\n",
    "print(f\"Your derivative of the cost function with respect to W^(1) connecting the first neuron of the input layer to the first neuron of the hidden layer is: {(d_l_w1[0][0]):.10f}\")\n",
    "print('Correct derivative of the cost function with respect to W^(1) connecting the first neuron of the input layer to the first neuron of the hidden layer is: 0.0006078749')\n",
    "print(f\"Your derivative of the cost function with respect to W^(0) connecting the first neuron of the input layer to the first neuron of the hidden layer is: {(d_l_w0[0][0]):.10f}\")\n",
    "print('Correct derivative of the cost function with respect to W^(0) connecting the first neuron of the input layer to the first neuron of the hidden layer is: -0.0000148791')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eef49f",
   "metadata": {},
   "source": [
    "As mentioned previously, to compute $\\frac{\\partial C}{\\partial W^{(l)}}$, we need to add all individual derivatives $\\frac{\\partial L_i}{\\partial W^{(l)}}$ for $i = \\{1,\\dots,N\\}$. We can easily do this with matrix multiplication by replacing the $1 \\times M$ vectors $y$ and $\\hat{y}$ with $N \\times M$ matrices $Y$ and $\\hat{Y}$. Here, we reuse the gradient calculation from the previous cell to implement the full `backpropagation()` function, which computes $\\frac{\\partial C}{\\partial W^{(1)}}$ and $\\frac{\\partial C}{\\partial W^{(0)}}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78072fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(Y,Y_hat,weights,h,o):\n",
    "    \n",
    "    delta = [np.empty((Y.shape[0],weights[0].shape[1])),np.empty((Y.shape[0],weights[1].shape[1]))]\n",
    "    dC_dw = [np.empty((weights[0].shape[0],weights[0].shape[1])),np.empty((weights[1].shape[0],weights[1].shape[1]))]\n",
    "    \n",
    "    # Calculate d(L,w1)\n",
    "    delta[1] = ...\n",
    "    dC_dw[1] = ...\n",
    "    \n",
    "    # Calculate d(L,w0)\n",
    "    delta[0] = ...\n",
    "    dC_dw[0] = ...\n",
    "    \n",
    "    return dC_dw\n",
    "\n",
    "# Test backpropagation for the entire training dataset\n",
    "dl_dw_index = backpropagation(Y,Y_hat,weights,h,o)\n",
    "\n",
    "print(f\"Your derivative of cost function with respect to W^(0) connecting the first neuron of the input layer to the first neuron of the hidden layer for the entire dataset is: {(dl_dw_index[0][0][0]):.3f}\")\n",
    "print('Correct derivative of cost function with respect to W^(0) connecting the first neuron of the input layer to the first neuron of the hidden layer for the entire dataset is: -3.043')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636e11b",
   "metadata": {},
   "source": [
    "### 5.4 Mini-batch gradient descent\n",
    "Batch gradient descent (GD) requires computing the gradient $\\frac{\\partial L_i}{\\partial W^{(l)}}$ for all $N$ samples in the training dataset before updating the weights. We employ a modified version of GD called mini-batch GD that uses non-overlapping subsets $\\mathbb{B}$ of the training set with $B$ samples instead of all $N$ samples to calculate the gradient and update the weights:\n",
    "$$\n",
    "W^{(l)}_{k} = W^{(l)}_{k-1}-\\eta\\sum_{i\\in \\mathbb{B}}\\frac{\\partial L_i}{\\partial W^{(l)}_{k-1}}\n",
    "$$\n",
    "At each step $k$ of mini-batch GD we use the model with the current set of internal parameters (weights in our example) to make predictions on $B$ samples, to compare the predictions to the real expected outcomes, to calculate the loss, and to use the loss to update the internal model parameters via backpropagation. Once all mini-batches are processed, a so-called training *epoch* has elapsed and we spit the training set into a new set of mini-batches to continue the training.\n",
    "\n",
    "From an implementation perspective, the training process is a nested for loop, where the outer loop iterates over the training epochs and the inner loop iterates of the mini-batches and updates the weights. Note that it is common to randomly shuffle the training samples at the end of each epoch to create different mini-batches in each epoch. In addition, we want to store and plot the training and test loss (averaged over training and test samples, respectively) and the training and test accuracy as a function of the number of epochs. These plots can help to diagnose whether the model is over- or underfitting, as well as whether various parameter choices are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mini-batch gradient descent\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "X_test = x_test_st \n",
    "Y_test = y_test\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# lists for saving the value of loss and accuracy of each epoch\n",
    "mean_train_loss_list = list()\n",
    "train_acc_list = list()\n",
    "mean_test_loss_list = list()\n",
    "test_acc_list = list()\n",
    "\n",
    "# Initialize the batch size, the number of epochs, and the learning rate\n",
    "batch_size = 500\n",
    "epochs = 20\n",
    "lr = 0.05\n",
    "\n",
    "# Initialize the weights\n",
    "weights = init_weights(layers)\n",
    "\n",
    "# Epoch for loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Initialize the layers\n",
    "    h,o = init_layer(layers,batch_size)\n",
    "   \n",
    "    # Initialize the training loss and accuracy for each epoch\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    # Create a random permutation for shuffling\n",
    "    shuffle = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Shuffle dataset and create mini-batches for each epoch\n",
    "    X_batches = np.array_split(X[shuffle],n_samples/batch_size)\n",
    "    Y_batches = np.array_split(Y[shuffle],n_samples/batch_size)\n",
    "    \n",
    "    # Mini-batch for loop\n",
    "    for b in range(int(n_samples/batch_size)):\n",
    "        \n",
    "        # Do feedforward step for mini-batch\n",
    "        Y_hat_batch= feedforward(X_batches[b],weights,o,h)\n",
    "        \n",
    "        # Evaluate loss and accuracy for mini-batch\n",
    "        train_loss += cross_entropy(Y_hat_batch,Y_batches[b])\n",
    "        train_acc += accuracy(labeling(Y_hat_batch),Y_batches[b])       \n",
    "    \n",
    "        # Do backpropagation step for mini-batch\n",
    "        dc_dw = backpropagation(Y_batches[b],Y_hat_batch,weights,h,o)\n",
    "\n",
    "        # Update weights\n",
    "        weights[1] = ...\n",
    "        weights[0] = ...\n",
    "        \n",
    "    # Evalute loss and accuracy for the training dataset per epoch\n",
    "    mean_train_loss = train_loss/len(Y_train)\n",
    "    train_acc = (train_acc/len(X_batches))\n",
    "    mean_train_loss_list.append(mean_train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    # Predictions for test dataset\n",
    "    h_test,o_test = init_layer(layers,n_test)\n",
    "    Y_hat_test= feedforward(X_test,weights,o_test,h_test)\n",
    "\n",
    "    # Evaluate loss and accuracy for the test dataset per epoch\n",
    "    mean_test_loss = cross_entropy(Y_hat_test,Y_test)/len(Y_test)\n",
    "    test_acc = accuracy(labeling(Y_hat_test), Y_test)\n",
    "    mean_test_loss_list.append(mean_test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    # Print statistics per epoch\n",
    "    print(f\"Epoch {epoch+1}: train_loss = {mean_train_loss:.3f} | train_acc = {train_acc:.3f} | test_loss = {mean_test_loss:.3f} | test_acc = {test_acc:.3f}\" )\n",
    "\n",
    "\n",
    "print(f\"\\nYour performance after {epochs} epoch(s) is: train_loss = {mean_train_loss:.3f} | train_acc = {train_acc:.3f} | test_loss = {mean_test_loss:.3f} | test_acc = {test_acc:.3f}\\n\")\n",
    "print('Correct performance after 20 epochs is: train_loss = 1.508 | train_acc = 0.923 | test_loss = 1.517 | test_acc = 0.892')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fced5b6",
   "metadata": {},
   "source": [
    "### 5.5 Loss and Accuracy curves\n",
    "\n",
    "In general, hyperparameters such as the number of epochs, the batch size, the learning rate, the number of hidden layers, and the number of neurons per layer need to be carefully tuned for each application. Here, we show how the loss and accuracy functions change as a function of the number of epochs with a given set of hyperparameters, but other hyperparameter values could also be explored and plotted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(mean_train_loss_list,label=\"Train loss\")\n",
    "ax[0].plot(mean_test_loss_list,label=\"Test loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(train_acc_list,label=\"Train accuracy\")\n",
    "ax[1].plot(test_acc_list,label=\"Test accuracy\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
